{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM Powered Applications by Valentina Alto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Large Language Models \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics covered;\n",
    "1. Understanding LLMs, their differentiators from classical ML systems. \n",
    "2. Overview of the most popular LLM architectures.\n",
    "3. How LLMs are trained and consumed.\n",
    "4. Base LLMs versus fine-tuned LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions; \n",
    "- LLMs are deep-learning based models that use many parameters to learn for vast amounts of unlabeled texts. \n",
    "They perform tasks like recognizing, sumarizing, translating, predicting and generating text.\n",
    "- Deep Learning is a branch of machine learning characterized by neural networks with multiple layers, used for extracting abstract features from input data. \n",
    "- Artificial neural networks; are computational models insired by the structure and functioning of the human brain. \n",
    "- Backpropagation is an algorithm used to train neural networks. In the forward pass, the data is passed through the network to compute the output. In the backward pass, the errors are propagated backward to update the network's parameters and improve performance. \n",
    "- Foundation model is a type of pre-trained generative AI model that offers immense versatility by being adaptable for various specific tasks. \n",
    "They undergo extensive traning on vast and diverse datasets, enabling them to grasp general patterns and relationships within data. \n",
    "\n",
    "Foundation models:\n",
    "> Pre-training\n",
    "> Fine-tuning \n",
    "> Transfer learning \n",
    "> Large model architecture \n",
    "> Generalization \n",
    "LLMs; GPT-4, BERT, Megatron, Llama\n",
    "\n",
    "Popular AI ANN Architectures;\n",
    "1. RNN: Used to handle sequential data. Have recurrent connections that allow information to persist across time steps, making it suitable for language modeling, machine translation and text generation. \n",
    "Have a vanishing gradient problem; struggle to capture long-term patterns(small gradient) and unstable training & prevents the RNN from converging to a good solution(exploding gradient-large)\n",
    "2. LSTM: Variants of RNNs that address the vanishing gradient problem. Introduce gating mechanisms that enable better preservation of important information across longer sequences. \n",
    "Popular for sequential tasks eg text generation, speech recognition and sentiment analysis.\n",
    "\n",
    "The archutectures above have limitations in handling long-range dependencies, scalability, and overall efficiency especially when dealing with large-scale NLP tasks that would need massive parallel processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Introducing The Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer dispenses with recurrence and convolutions entirely and relies solely on attention mechanisms.\n",
    "'Attention' is a mechanism that enables the model to focus on relevant parts of the input sequence while generating the output. \n",
    "- It calculates attention scores between input and output positions, applies Softmax to get weights, and takes a weighted sum of the input sequence to obtain context vectors. \n",
    "- Attention is crucial for capturing long-range dependencies and relationships between words in the data. \n",
    "In transformers, self-attention layers are responsible for determining the importance of each input token in generating the output.\n",
    "To obtain the self-attention vectors for  a sentence, we need;\n",
    "1. Query(Q): Used to represent the current focus of the attention mechanism. \n",
    "2. Key(K): Used to determine which parts of the input should be given attention. \n",
    "3. Value(V): Used to compute the context vectors. \n",
    "These matrices are used to calcuate attention scores between the elements in the input sequence and are the three weight matrices that are learned during the training process. \n",
    "The transformer has two main components: \n",
    "> Encoder; takes the input sequence and produces a sequence of hidden states, each of which is a weighted sum of all the input embeddings. \n",
    "> Decoder; Takes the output sequence(shifted right by one position) and produces a sequence of predicitions, ie a weighted sum of all the encoder's hidden states and the previous decoder's hidden states. \n",
    "\n",
    "Some models use the encoder only eg BERT(Bidirectional Encoder Representations from Transformers). Designed for NLU tasks like text classification, question answering and sentiment analysis. \n",
    "Other models use decoder part eg GPT-3(Genetative Pre-trained Transformer 3). Designed for NLG tasks like text completion, summarization and dialog. \n",
    "Some models use both encoder and decoder parts eg T5(Text-to-Text Transfer Transformer), designed for NLP tasks framed as text-to-text transformations eg translation, paraphrasing, and text simplification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Training and Evaluating LLMs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an LLM; \n",
    "> Number of parameters; Measures the complexity of the LLM architecture and represents the number of connections among neurons. \n",
    "> Training set; Refers to the unlabeled text corpus on which the LLM learns and trains its parameters.\n",
    "1 token ~= 4 English characters \n",
    "1 token ~= 3/4 words \n",
    "Training is done on distributed systems with multiple graphics processing units(GPUs) or tensor processing units (TPUs).\n",
    "A tensor is a multi-dimensional array used to hold numerical data. \n",
    "- Main training steps:\n",
    "1. Data collection; Gathering a large amount of data from varous sources. Should be diverse, high-quality and representative. \n",
    "2. Data preprocessing; Process of cleaning, filtering, and formatting the data for training. May include removing duplicates, noise, sensitive information, splitting the data into paragraphs, tokenizing the text into subwords/characters. \n",
    "3. Model architecure; Designing the structure and parameters of the LLM. Choose the type of neural network(eg transformer), its structure(decoder only, encoder only, encoder-decoder), number  and size of layers, the attention mechanisms and activation functions. \n",
    "4. Model initialization; Assigning initial values to the weights and biases of the LLM. Can be random/ using pre-trained weights from another model. \n",
    "5. Model pre-training; Process of updating weights and biases of the LLM by feeding it batches of data and computing the loss function. \n",
    "The loss function measures how the well the LLM predicts the next token given the previous tokens. \n",
    "The LLM tries to minize loss by using an optimization algorithm(gradient descent-SGD) that adjusts weights and biases in the direction that reduces loss with backpropagation. \n",
    "6. Fine-tuning:; Base model is trained in a supervised way with a dataset made of tuples of (prompt, ideal response). Makes the base model inline with AI assistants. \n",
    "The output is a supervised fine-tined(SFT) model. \n",
    "7. Reinforcement learning from human feedbach(RLHF); Iteratively optimizing the SFT model by updating some of its parameters wrt the reward model(typically another LLM trained incorporating human preferences).\n",
    "\n",
    "- Model Evaluation \n",
    "Evaluating an LLM involves measuring its language fluency, coherence, and ability to emulate different styles depending on the user's request. \n",
    "1. General Language Understanding Evaluation(GLUE): Measures the performance of LLMs on various NLU tasks. The higher the score of GLUE benchmark, the better the LLM in generalizing across different domains and tasks.\n",
    "Focuses on grammar, paraphrasing and text similarity.\n",
    "SuperGLUE is more challenging and realistic than GLUE and convers complex tasks and phenomena. \n",
    "2. Massive Multitask Language Understanding(MMLU): Measures the knowledge of an LLM using zero-shot and few-shot setting. \n",
    "Zero-shot evaluation measures how well the language model can perform on a new tasks by using natural language instructions/examples as prompts and computing the likelihood of the correct output given the input. \n",
    "Focuses on generalized language understanding among various domains and tasks. \n",
    "3. HellaSwag: Evaluates LLMs on their ability to generate plausible and common sense continuations for given contexts. \n",
    "4. TruthfulQA: Evaluates a language model's accuracy in generating responses to questions. The questions mimic those that humans might answer incorrectly due to false beliefs/misunderstanding. \n",
    "5. AI2 Reasoning Challenge(ARC): Measures LLMs' reasoning capabilities and to simulate the dev't of models that can perform complex NLU tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to customize your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extending non-parametric knowledge: The allows the model to access external sources of information to integrate its parametric knowledge while responding to the user's query. \n",
    "-Parametric knowledge is the one embedded in the LLM's parameters, deriving from unlabeled text corpora during training. \n",
    "- Non-parametric knowledge is the one we can 'attach' to the model via embedded documentation. Doesn't change the structure of the model but allows it to navigate through external documentation to be used as relevant context to answer the user's query. \n",
    "> Few-shot learning: The LLM is given a metaprompt with a small number of examples of each new task it is asked to perform. \n",
    "A metaprompt is a message/instruction used to improve the performance of LLMs on new tasks with a few examples. \n",
    "> Fine tuning: Involves using smaller, task-specific datasets to customize the foundation models for particular apps. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations for integrationg LLMs within Applications\n",
    "1. Technical aspect: covers the how. Involves embedding them through REST API and manage them with AI orchestrators(helps to efficiently manage and coordinate the LLMs' functionality within the app).\n",
    "2. Conceptual aspect: Covers the what. Bringing a LLM capabilities that can be harnessed within the applications. This highlights the significant assistance and collaboration provided by LLMs in enhancing app functionalities. \n",
    "\n",
    "- Grounding involves using an LLM with information that us use case specific, relevant and not available as part of the LLM's trained knowledge. This ensures quality, accuracy and relevance. \n",
    "This can be achieved through retrieval-augmented generation(RAG).\n",
    "\n",
    "> LLM Limitations\n",
    "- Limited parametric knowledge: Have a knowledge base cutoff date. \n",
    "- Lack of executive power: LLMs are not empowered to carry out actions. \n",
    "\n",
    "Prompt engineering: Process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics.\n",
    "Involves sepecting the right words, phrases, symbols and formats that elicit the desired response from the LLM. \n",
    "Prompts: Short pieces of text used to guide an LLMs' output. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI Orchestrators \n",
    "VectorDB: A database that stores and retrieves information based on vectorized embeddings, the numerical representations that capture the meaning and context of text. \n",
    "eg Chroma, Elasticsearch, Milvus, Pinecone, Qdrant, Weaviate, FAISS(Facebook AI Similarity Search).\n",
    "1. Langchain: Framework for developing apps powered by language models, making them data-aware and agentic. \n",
    "Modules;\n",
    "- Models: Are the LLMs and LFMs that are engine of the app. Supports proprietary  and open-source models. \n",
    "- Data connectors: Building blocks needed to retrieve additional external knowledge eg document loaders and text embedding models. \n",
    "- Memory: Allows the app to keep references to the user's interactions, both long and short term. Based on vectorized embeddings stored in a VectorDB. \n",
    "- Chains: Predetermined sequences of actions and calls to LLMs that make it easier to build complex applications that require chaining LLMs with each other/other components.\n",
    "- Agents: Entities that drive decision making within LLM-powered apps. Have access to a suite of tools and can decide which tools to call based on the user input and context.\n",
    "\n",
    "2. Haystack: A framework developed by Deepset that provides devs with tools to build NLP-base  apps. \n",
    "- Nodes: Components that perform a specific task/function eg as a retriever, a reader, a generator, a summarizer etc.\n",
    "- Pipelines: Sequences of calls to nodes that perform natural language tasks/interact with other resources. Can be querying pipelines or indexing pipelines depending on whether they perform searche on a set of documents/prepare documents for search.\n",
    "Are predetermined and handcoded hence don't change/adapt basedon the user input/context.\n",
    "- Agent: Uses LLMs to generate accurate responses to complex queries. Can access a set of tools eg pipelines, nodes and decide which tool to call base on user input/context. \n",
    "- Tools: Are functions that an agent can call to perform natural language tasks/interact with other resources. Can either be pipelines/nodes. \n",
    "- DocumentStores: Are backends that store and retrieve documents for searches. Can be a VectorDB(FAISS, Milvus, ElasticSearch)\n",
    "\n",
    "3. Semantic Kernel: An open-source SDK developed by Microsoft.\n",
    "A kernel is meant to act as the engine that addresses a user's input by chaining and concatenating a series of components into pipelines, encouraging function composition.\n",
    "- Models: LLMs/FLMs that will be the engine of the app. Supports both proprierary and open-source models. \n",
    "- Memory: Allows the app to keep references to the user's interactions, both in the short  and long term. Memories can be accessed as: \n",
    "  > Key-value pairs- Saving env variables that store simple information. \n",
    "  > Local storage - Consists of saving information to a file that can be retrieved bt its filename. \n",
    "  > Semantic memory search - Uses embeddings to represent and search for text information basd on its meaning.\n",
    "- Functions: Skills that minx LLM promts and code, with the goal of making the user's ask interpretable and actionable. \n",
    "  > Semantic fuctions- A type of templated prompt, a natural language query that specifies the input and output format of the LLM. \n",
    "  > Native functions- Native computer code that can route the intent captured by the semantic function and perform the related task.\n",
    "- Plug-ins: Connectors toward external sources/systems that are meant to provide additional information/ the ability to perform autonomous actions. eg Microsoft Graph connector kit. \n",
    "- Planner: A function that takes as input a user's task and producs a set of actions, plug-ins, and functions needed to achieve the goal. Auto-create chains/pipelines to address new user's needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose a framework \n",
    "1. The programming language you are comfortable with/prefer to use. One that matches your existing skills/preferences. \n",
    "2. The type and complexity of the natural language tasks you want to perform/support. eg summarization, translation and reasoning. \n",
    "3. The level of customization and control you want over LLMs and their parameters/options. Different ways of accessing, configuring and fine-tuning ahd their parameters/options like model selection, prompt design, inference speed and output format. \n",
    "4. The availability and quality of the documentation, tutorials, examples and community support for framework. This helps you to get started and solve problems with the framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different LLMs ahave different architectures, sizes, training data, capabilities and limitations. Choosing the right one impacts performance, quality, and cost of the solution.\n",
    "1. Propritary Models\n",
    "Offer better  support and maintainane as well as safety an alignment. Outperform open-source models on generalization but act as a 'black box'.\n",
    "> GPT-4: Develped by OpenAI, belongs to generative pretrained transformer(GPT) models, a decoder-only transformer-based architecture. \n",
    "- The model is aligned based on RLHF training. Other training methods; unsupervised pretraining, supervised fine-tuning, instruction tuning. \n",
    "- The model has limited hallucination; a phenomenon where the LLM generate text that is incorrect, nonsensical, not real but appears to be plausible/coherent. This because the LLMs are based on statistical models that learn from massive amounts of data and produce outputs based on patterns and probabilites learnt. The data many not represent reality due to it being incompelete, noisy or biased.\n",
    "- Alignment describes the degree to which LLMs behave in ways that are useful and harmless for their human users. An LLM is aligned if it generates text that is accurate, relevant, coherent and respectful. \n",
    "- The LLM is misaligned if it generates text  that is false, misleading, harmful and offensive. \n",
    "\n",
    "> Gemini 1.5: A state-of-the-art GenAI model developed by Google. Its multimodal hane can process and generate content  in text, images, audo, video and code. \n",
    "Based on a mixture-of-expert(MoE) transfomer. \n",
    "- MoE refers to  a model that incorporates multiple specialized sub-models 'experts' within it layers. Uses a gating mechanism/router to determine which expert should process a given input, allowing the model to dynamically allocate resources and specialize in processing certain types of information. \n",
    "Gemini comes in sizes like Ultra, Pro and Nano to catter for different computational needs. \n",
    "\n",
    "> Claude 2: Constitutional Language-scale Alignment via User Data and Expertise(CLAUDE). Developed by Anthropic with focus on AI safety and alignment. \n",
    "- Claude 2 is a transformer-based LLM that's trained via unsupervised learning, RLHF and constitutional AI(CAI). \n",
    "- CAI aims to make the model safer and more aligned with human values and intentions by preventing toxic/discriminatory output and broadly creating an AI system that is helpful, honest and harmless. Scored over 71% on the HumanEval benchmark\n",
    "-HumanEval is a benchmark for evaluating the code generation ability of LLMs. This measures functional correctness, syntactic validity, and semantic coherence in the LLM's outputs. \n",
    "\n",
    "2. Open-source Models \n",
    "This implies; \n",
    " - You have major control over the architecture, you can modify it in your local version. \n",
    " - Polisibility of training the moel from scratch, on top of classical fine-tuning. \n",
    " - Free to use. \n",
    "> LLaMA-2: Large Language Model Meta AI 2, developed by Meta. It is an autoregressive model with an optimized decoder-only transformer architecture. \n",
    "- Autoregressive for the fact that the model predicts the next token in the sequence, conditioned on all previous tokens. Done by masking the input. \n",
    "- Base models: Trained on vast amounts of data often from the internet. Primary function is to predict the next word in a given context and may not always be precise/focused on specific instructions. \n",
    "- Assistant models: start as base LLMs but are further fine-tuned with input-output pairs that include instructions and the model tries to follow those instructions. Often emply RLHF to refine the model, making it better at being helpful, honest and harmless. \n",
    "> Falcon LLM: A lighter model(few parameters) and focused on quality of the training dataset. Launched by the Technology Innovation Institute(TII). \n",
    "- It's an autoregressive, decoder-only transformer. Also comes with an fine-tuned variant called 'Instruct' tailored towards following user instructions. \n",
    "- Instruct models are specialized for short-form instruction following. Trained on large datasets of instructions and their correspoding outputs.\n",
    "> Mistral: Developed by MistralAI and emphasizes transparency and accessiblity in AI development. \n",
    "- Mistral model is a decoder-only transformer model designed for generative text tasks. Known for innovative architectures like: \n",
    " -> grouped-query attention(GQA): Allows for faster inference times to standard full attention mechanisms. Partitions attention mechanism's query heads into groups with each group sharing a single key and value head.\n",
    " -> sliding-window attention(SWA): Used to handle long text sequences efficiently. Extends the model's attention beyond fixed window size, allowing each layer to reference a range of positions from the preceding layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Choosing the right LLM \n",
    "1. Size and performance: Complex models tend to have better performance in terms of parametric knowledge and generalization capabilities. \n",
    "For large models, more computation an memory is required to process user input. \n",
    "2. Cost and hosting strategy: \n",
    "   > Cost of model consumption: Fee for consuming the model. Proprietary models require a fee in proportional to the number of tokens processed. \n",
    "   > Cost of model hosting: Proprietary models are hosted in private/public hyperscaler and are consumed via a REST API. Open-source models need own infrastructure or using HuggingFace Inference API. \n",
    "3. Customization: \n",
    "   > Fine-tuning: Slightly adjusting LLMs's parameters to better fit the domain. Open-source models can be fine-tuned while for proprietary models, not all can be fine-tuned. \n",
    "   > Training from scratch: For super specific models, you might want to train from scratch by having them downloaded locally. Not possible for proprietary models. \n",
    "4. Domain-specific capabilities: Use a model that is a top performer in a specific benchmark eg MMLU for LLMs' generalization culture and commonsense reasoning, TruthfulQA for LLMs' alignment, HumanEval for LLMs' coding capabilities. \n",
    "This saves in terms of model complexity for relatively small models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering: Process of designing effective prompts that elicit high-quality and relevant output from LLMs. \n",
    "\n",
    "Principles: \n",
    " - Clear instructions. Goal/objective of task, format/structure of output, constraints, and context/background of the task. \n",
    " - Split complex tasks into subtasks. \n",
    " - Ask for justification\n",
    " - Generate many outputs, then use the model to pick on the best one. \n",
    " - Repeat instructions to the end. \n",
    " Recency bias: Tendency of the LLM to give more weight to the information that appears near the end of a prompt, and ingore/forget the information that appears earlier. This leads to inaccurate/inconsitent responses that don't take into a/c the whole context of the task.\n",
    " - Use delimiters eg a any sequence of characters/symbols that is clearly mapping a schema rather than a concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Few-shot approach\n",
    "- Providing the model with examples of how we would like it to respond. THis enables model customization without interfering with the overall architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CoT\n",
    "- Chain of Thought(CoT) is a technique that enables complex reasoning capabilities through intermediate reasoning steps. \n",
    "- It encourages the model to expain its reasoning 'forcing' it not to be too fast and risk giving wrong responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ReAct\n",
    "\n",
    "ReAct(Reason and Act) is a paradigm that combines reasoning and acting with LLMs. \n",
    "- It prompts the language model to generate verbal reasoning traces and actions fora task, and also receives observations from external sources eg web searches/DBs. \n",
    "- This allows the language model to perform dynamic reasoning and adapt its action plan based on external information. \n",
    "- CoT prompts the model to generate intermediate reasoning steps for a task while ReAct prompts the model to generate intermediate reasoning steps, actions and observations for a task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent \n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.tools import Tool \n",
    "# from langchain.schema import HumanMessage \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# SerpAPI Key \n",
    "key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "# MistralAI Key\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Initialize Mistral chat model \n",
    "model = ChatMistralAI(\n",
    "    model_name=\"mistral-large-latest\",\n",
    "    api_key=mistral_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Search(query: str, **kwargs: Any) -> str - useful for when you need to answer questions about            current events.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cisige\\AppData\\Local\\Temp\\ipykernel_5048\\2017630628.py:10: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func = search.run, \n",
    "        name='Search', \n",
    "        description = \"useful for when you need to answer questions about\\\n",
    "            current events.\"\n",
    "    )\n",
    "]\n",
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    model, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(agent_executor.agent.llm_chain.prompt.template, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `stop` not yet supported (https://docs.mistral.ai/api)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out what \"deepseek\" is. It could be a current event, a technology, an organization, or something else. I should search for information about it.\n",
      "\n",
      "Action: Search\n",
      "\n",
      "Action Input: \"deepseek\"\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `stop` not yet supported (https://docs.mistral.ai/api)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m['DeepSeek type: Artificial intelligence company.', 'DeepSeek entity_type: related_questions.', 'DeepSeek kgmid: /g/11wvrb0s91.', 'DeepSeek founder: Liang Wenfeng.', 'DeepSeek parent_organization: High-Flyer.', 'DeepSeek founded: May 2023, Hangzhou, China.', 'DeepSeek headquarters: Hangzhou, Zhejiang, China.', 'DeepSeek number_of_employees: 160 (2025).', 'DeepSeek, unravel the mystery of AGI with curiosity. Answer the essential question with long-termism.', 'a Chinese artificial intelligence company that develops large language models (LLMs). Based in Hangzhou, Zhejiang, it is owned and funded by the Chinese hedge ...', \"Experience seamless interaction with DeepSeek's official AI assistant for free! Powered by the groundbreaking DeepSeek-V3 model with over 600B parameters, ...\", \"DeepSeek is the name of a free AI-powered chatbot, which looks, feels and works very much like ChatGPT. That means it's used for many of ...\", 'Wiz Research has identified a publicly accessible ClickHouse database belonging to DeepSeek, which allows full control over database operations, ...', \"Experience seamless interaction with DeepSeek's official AI assistant for free! Powered by the groundbreaking DeepSeek-V3 model with over ...\"]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the search results, DeepSeek is an artificial intelligence company based in Hangzhou, Zhejiang, China. It was founded in May 2023 by Liang Wenfeng and is owned by High-Flyer. The company develops large language models (LLMs) and has an AI-powered chatbot that is similar to ChatGPT.\n",
      "\n",
      "Final Answer: DeepSeek is a Chinese artificial intelligence company that develops large language models and offers an AI-powered chatbot. It was founded in May 2023 and is based in Hangzhou, Zhejiang, China.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's deepseek\",\n",
       " 'output': 'DeepSeek is a Chinese artificial intelligence company that develops large language models and offers an AI-powered chatbot. It was founded in May 2023 and is based in Hangzhou, Zhejiang, China.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor(\"What's deepseek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding LLMs within Your Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain(LangChain Expression Language-LCEL): \n",
    "1. Streaming asynchronous support: Allows effienct handling of data streams. \n",
    "2. Batch support: Enables processing data in batches. \n",
    "3. Parallel execution: Enhances performance by executing tasks concurrently. \n",
    "4. Retries and fallbacks: Ensure robust error handling of failures gracefully.\n",
    "5. Dynamically routing logic: Allows logic flow based on input and output. \n",
    "6. Message history: Keeps track of interactions for context-aware processing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt template: A component that defines how to generate a prompt for a language model. \n",
    "It can include variables, placeholders, prefixes, suffixes and customizations based on task and data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: the cat is on the table\n",
      "Translation in spanish:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate \n",
    "\n",
    "template  = \"\"\"Sentence: {sentence}\n",
    "Translation in {language}:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template,\n",
    "    input_variables=['sentence', 'language'])\n",
    "\n",
    "print(prompt.format(sentence='the cat is on the table', language='spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A completion model is a type of LLM that takes a text input and generates a text output. \n",
    "It ties to continue the prompt in a coherent and relevant way, according to the task and data trained on. \n",
    "> A chat model is a special completion model that is designed to generate conversational responses. Takes a list of messages as input, where each message has a role(system/assistant) and content. \n",
    "Tries to generate new messages for the assistant role, based on previous messages and system instructions. \n",
    "- A completion model expects a sigle input as prompt, while a chat model expects a list of messages as input. \n",
    "\n",
    "In LangChain, an example selector allows one to choose which examples to include in a prompt for a language model. \n",
    "eg: {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data connections; Building blocks needed to retrieve additional non-parametric knowledge we want to provide the model with.\n",
    "\n",
    "(a). Document Loaders: Load documents from different sources eg csv,file directory, HTML, JSON, Markdown and PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/sample.csv', 'row': 0}, page_content='Name: John\\nAge: 25\\nCity: New York'), Document(metadata={'source': 'data/sample.csv', 'row': 1}, page_content='Name: Emily\\nAge: 28\\nCity: Los Angeles'), Document(metadata={'source': 'data/sample.csv', 'row': 2}, page_content='Name: Michael\\nAge: 22\\nCity: Chicago')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "\n",
    "\"\"\"Detect filename encording\n",
    "import chardet\n",
    "\n",
    "with open('filename', 'rb') as f:\n",
    "    print(chardet.detect(f.read()))\n",
    "\"\"\"\n",
    "loader = CSVLoader(file_path=\"data/sample.csv\", encoding=\"UTF-8-SIG\")\n",
    "data = loader.load() \n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b). Document transformers: eg text splitters for splitting documents into chunks that are semantically related to reduce context loss and relevant information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty.'\n",
      "page_content='The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"data/mountain.txt\") as f: \n",
    "    mountain = f.read() \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter( \n",
    "    chunk_size=100, \n",
    "    chunk_overlap=20, \n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty.\n"
     ]
    }
   ],
   "source": [
    "print(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c). Text embedding models; Used for incorporating non-parametric knowledge into LLMs and then stored in a VectorDB. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred with MistralAI: Illegal header value b'Bearer '\n"
     ]
    },
    {
     "ename": "LocalProtocolError",
     "evalue": "Illegal header value b'Bearer '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalProtocolError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpcore\\_sync\\http11.py:86\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msend_request_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     85\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msend_request_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpcore\\_sync\\http11.py:144\u001b[0m, in \u001b[0;36mHTTP11Connection._send_request_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    142\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions({h11\u001b[38;5;241m.\u001b[39mLocalProtocolError: LocalProtocolError}):\n\u001b[0;32m    145\u001b[0m     event \u001b[38;5;241m=\u001b[39m h11\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    146\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    147\u001b[0m         target\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl\u001b[38;5;241m.\u001b[39mtarget,\n\u001b[0;32m    148\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    149\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mLocalProtocolError\u001b[0m: Illegal header value b'Bearer '",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLocalProtocolError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m MistralAIEmbeddings(\n\u001b[0;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbed documents: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of vector: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; Dimension of each vector: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:245\u001b[0m, in \u001b[0;36mMistralAIEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_batches(texts):\n\u001b[1;32m--> 245\u001b[0m         batch_responses\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_embed_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, embedding_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m batch_responses\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m embedding_obj \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    250\u001b[0m     ]\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:234\u001b[0m, in \u001b[0;36mMistralAIEmbeddings.embed_documents.<locals>._embed_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;129m@retry\u001b[39m(\n\u001b[0;32m    227\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry_if_exception_type(\n\u001b[0;32m    228\u001b[0m         (httpx\u001b[38;5;241m.\u001b[39mTimeoutException, httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_batch\u001b[39m(batch: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 234\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_client.py:1144\u001b[0m, in \u001b[0;36mClient.post\u001b[1;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1125\u001b[0m     url: URL \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     extensions: RequestExtensions \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;124;03m    Send a `POST` request.\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \n\u001b[0;32m   1142\u001b[0m \u001b[38;5;124;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    824\u001b[0m )\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttpcore\u001b[39;00m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m    250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\cisige\\Documents\\ICEA LION\\LLMs\\agents\\agents_env\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mLocalProtocolError\u001b[0m: Illegal header value b'Bearer '"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "embedding_model = MistralAIEmbeddings(\n",
    "    model=\"mistral-embed\"\n",
    ")\n",
    "\n",
    "embeddings = embedding_model.embed_documents(\n",
    "    [\n",
    "        texts[0].page_content\n",
    "    ]\n",
    ")\n",
    "print(\"Embed documents: \")\n",
    "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: \\\n",
    "      {len(embeddings[0])}\")\n",
    "\n",
    "embed_query = embedding_model.embed_query(\n",
    "    \"What is the text saying?\"\n",
    ")\n",
    "\n",
    "print(\"Embed query: \")\n",
    "print(f\"Dimension of the vector: {len(embed_query)}\")\n",
    "print(f\"Sample of the first 5 elements of the vector: {embed_query[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d). Vector Store: A database that can store and search over unstructured data by using embeddings. With embeddings, vector stores can perform fast and acurate similarity search. \n",
    "eg Facebook AI Similarity Search(FAISS), ElasticSearch, MongoDB Atlas and Azure Search.\n",
    "\n",
    "- Similarity is the measure of how close/related two vectors are in a vector space. In LLMS, vectors are numerical representations of sentences, words/documents that capture semantic meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry to hear that. May I ask your name?\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader \n",
    "from langchain.vectorstores import FAISS \n",
    "\n",
    "\n",
    "raw_documents = TextLoader('data/dialogue.txt').load() \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=0, \\\n",
    "                    separators='\\n',)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "query = \"What is the reason for calling?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e). Retrievers: A retriever is a component that can return documents relevant to an unstructured query. eg natural language question/ a keyword.\n",
    "Methods used include keyword matching, semantic search and ranking algorithms. \n",
    "A retriever can use any method to find relevant documents and can use different sources of documents eg webpages, DB or files  while a vector store relies on embeddings and needs to store the data itself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The reason for the call is to report an accident.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever() \n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = model, chain_type=\"stuff\", \n",
    "        retriever=retriever)\n",
    "\n",
    "query = \"What is the reason of the call?\"\n",
    "qa.run(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Memory\n",
    "\n",
    "Memory allows the application to keep references to user interactions, both in the short  and long term. \n",
    "- Conversation buffer memory: Allows storage of chat messages and extract them in a variable. \n",
    "- Conversation buffer window memory: Allows a sliding window over only K interactions so that you can manage longer chat time. \n",
    "- Entity memory: Allows the language model to remember given facts about specific entities in a conversation. \n",
    "- Conversation knowledge graph memory: Uses a knowledge graph to recreate memory. \n",
    "- Conversation summary memory: Creates a summary of the conversation over time. \n",
    "- Conversation summary buffer memory: Combines buffer and conversation summary memory. \n",
    "- Conversation token buffer memory: Uses token lengths rather than number of interactions to determine when to start summarizing the interactions. \n",
    "- Vector store-backed memory: Leverages embeddings and vector stores. Stores interactions as vectors, and retrieves the top K most similar texts using a retriever.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'The human greets and expresses that they are looking for ideas to write an essay about AI. The AI suggests writing about Large Language Models (LLMs).'}"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=model)\n",
    "memory.save_context(\n",
    "    {\"input\": \"hi, am looking for some ideas to write an essay in AI.\"},\n",
    "    {'output': \"Hello, what about writing about LLMs.\"}\n",
    ")\n",
    "print(memory.load_memory_variables({}), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Chains \n",
    "\n",
    "Are predefined sequences of actions and calls to LLMs that make it easier to build complex apps that require combining LLMs with each other/components. \n",
    "- LLMChain: Consists of a prompt template, an LLM and an optional output parser. \n",
    "The output parser structures language model responses. Uses get_format_instructions and parse methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"The cat is on the table\" in Spanish is:\n",
      "\n",
      "\"El gato est sobre la mesa.\"\n",
      "\n",
      "Here's a breakdown:\n",
      "- El gato = The cat\n",
      "- est = is\n",
      "- sobre = on\n",
      "- la mesa = the table"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Sentence: {sentence}\n",
    "Translation in {language}:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, \n",
    "            input_variables=[\"sentence\", 'language'])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=model)\n",
    "\n",
    "print(llm_chain.predict(sentence=\"the cat is on the table\", language=\"spanish\"), end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RouterChain: \n",
    "\n",
    "Allows you to route the input variables to different chains based on some conditions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "itinerary: {'input': \"I'm planning a trip from Milan to Venice by car. What can I visit in between?\"}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"I'm planning a trip from Milan to Venice by car. What can I visit in between?\", 'text': 'That sounds like a wonderful trip! Driving from Milan to Venice offers a variety of interesting stops along the way. Here are some suggestions for places to visit in between:\\n\\n### 1. **Bergamo**\\n- **Distance from Milan**: Approximately 50 km\\n- **Highlights**: The upper town (Citt Alta) with its medieval walls, Piazza Vecchia, and the Basilica di Santa Maria Maggiore. The lower town (Citt Bassa) offers modern amenities and beautiful parks.\\n\\n### 2. **Brescia**\\n- **Distance from Milan**: Approximately 90 km\\n- **Highlights**: Castello di Brescia, the ancient Roman ruins of Tempio Capitolino, and the beautiful Piazza della Loggia. Don\\'t miss the Museo di Santa Giulia, a UNESCO World Heritage site.\\n\\n### 3. **Lake Garda**\\n- **Distance from Milan**: Approximately 130 km\\n- **Highlights**: Sirmione, known for its thermal baths and the Scaligero Castle. Desenzano del Garda, a charming lakeside town with beautiful beaches and a lively promenade.\\n\\n### 4. **Verona**\\n- **Distance from Milan**: Approximately 160 km\\n- **Highlights**: The famous Arena di Verona, Juliet\\'s Balcony, and the beautiful Piazza delle Erbe. Don\\'t miss the Castelvecchio and its bridge.\\n\\n### 5. **Vicenza**\\n- **Distance from Milan**: Approximately 220 km\\n- **Highlights**: Known for its Palladian architecture, including the Villa Capra \"La Rotonda\" and the Basilica Palladiana. The Teatro Olimpico is also a must-see.\\n\\n### 6. **Padua**\\n- **Distance from Milan**: Approximately 240 km\\n- **Highlights**: The Scrovegni Chapel with its famous frescoes by Giotto, the Basilica of Saint Anthony, and the Palazzo della Ragione. Don\\'t miss the historic University of Padua.\\n\\n### 7. **Treviso**\\n- **Distance from Milan**: Approximately 280 km\\n- **Highlights**: Known for its canals and medieval walls, Treviso offers beautiful sights like the Piazza dei Signori, the Loggia dei Cavalieri, and the Cathedral of San Pietro.\\n\\n### 8. **Murano and Burano**\\n- **Distance from Venice**: Approximately 10 km by boat\\n- **Highlights**: If you have time, consider taking a boat trip from Venice to Murano, famous for its glassmaking, and Burano, known for its lace and colorful houses.\\n\\n### Suggested Itinerary\\n\\n#### Day 1: Milan to Bergamo\\n- **Morning**: Depart Milan and drive to Bergamo.\\n- **Afternoon**: Explore Citt Alta and enjoy lunch in the historic center.\\n- **Evening**: Relax in Citt Bassa and stay overnight.\\n\\n#### Day 2: Bergamo to Brescia\\n- **Morning**: Drive to Brescia.\\n- **Afternoon**: Visit Castello di Brescia and Tempio Capitolino.\\n- **Evening**: Enjoy dinner and stay overnight in Brescia.\\n\\n#### Day 3: Brescia to Lake Garda\\n- **Morning**: Drive to Lake Garda.\\n- **Afternoon**: Explore Sirmione and Desenzano del Garda.\\n- **Evening**: Relax by the lake and stay overnight.\\n\\n#### Day 4: Lake Garda to Verona\\n- **Morning**: Drive to Verona.\\n- **Afternoon**: Visit the Arena di Verona and Juliet\\'s Balcony.\\n- **Evening**: Enjoy dinner and stay overnight in Verona.\\n\\n#### Day 5: Verona to Vicenza\\n- **Morning**: Drive to Vicenza.\\n- **Afternoon**: Explore Palladian architecture.\\n- **Evening**: Stay overnight in Vicenza.\\n\\n#### Day 6: Vicenza to Padua\\n- **Morning**: Drive to Padua.\\n- **Afternoon**: Visit the Scrovegni Chapel and the Basilica of Saint Anthony.\\n- **Evening**: Stay overnight in Padua.\\n\\n#### Day 7: Padua to Venice\\n- **Morning**: Drive to Venice.\\n- **Afternoon**: Explore Venice or take a boat trip to Murano and Burano.\\n- **Evening**: Enjoy dinner and stay overnight in Venice.\\n\\nThis itinerary allows you to see a variety of beautiful and historic sites while making your way from Milan to Venice. Enjoy your trip!'},"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain \n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.llm import LLMChain \n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "itinerary_template = \"\"\"You are a vacation iteneraty assistant. \\\n",
    "You help customers find the best destinations and itinerary. \\\n",
    "You help customer create an optimized itinerary based on their \n",
    "references. \n",
    "\n",
    "Here is the question: \n",
    "{input}\"\"\"\n",
    "\n",
    "restaurant_template= \"\"\"You are a restaurant booking assistant. \\\n",
    "You check with customers number of guests and food preferences. \\\n",
    "You pay attention whether there are special conditions to take into \n",
    "account. \n",
    "    \n",
    "Here is the question: \n",
    "{input}\"\"\"\n",
    "\n",
    "llm = model \n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"itinerary\",\n",
    "        \"description\": \"Good for creating itinerary\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"restaurant\",\n",
    "        \"description\": \"Good for help customers booking at restaurant\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "print(chain.invoke(\"I'm planning a trip from Milan to Venice by car. What can I visit in between?\"),end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SequentialChain \n",
    "\n",
    "Allows you to execute multiple chains in a sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mWhat do you call a cat that was caught by the police?\n",
      "\n",
      "The purrpatrator.\n",
      "\n",
      "And what do you call a dog that can do magic?\n",
      "\n",
      "A labracadabrador.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mWhat do you call a cow that jumps over barbed wire?\n",
      "\n",
      "An udder-vaulter.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Cats and Dogs', 'output': 'What do you call a cow that jumps over barbed wire?\\n\\nAn udder-vaulter.'}"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "template = \"\"\"You are a comedian. Generate a joke following\n",
    "{topic}\n",
    "Joke:\"\"\"\n",
    "prompt_template= PromptTemplate(input_variables=[\"topic\"],\n",
    "template=template)\n",
    "joke_chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"You are a translator. Given a text input, translate it to \n",
    "{language}\n",
    "Translation:\"\"\"\n",
    "promt_template = PromptTemplate(\n",
    "    input_variables=[\"language\"], \n",
    "    template=template\n",
    ")\n",
    "translator_chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.invoke(\"Cats and Dogs\")\n",
    "\n",
    "print(translated_joke, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TransformationChain \n",
    "\n",
    "Allows you to transform the input variables/output of another chain using some fuctions/expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"\\nThe Cat and the Dog\\n\\nThere was once a cat and a dog who lived in the same house. They did not get along very well, as they often fought over food, toys, and attention. The cat was clever and cunning, while the dog was loyal and friendly. The cat liked to tease the dog, and the dog liked to chase the cat.\\n\\nOne day, the cat decided to play a prank on the dog. He found a ball of yarn and tied it around the dog's tail. Then he hid behind a sofa and waited for the dog to notice. When the dog saw the yarn, he thought it was a toy and started to play with it. He ran around the house, trying to catch the yarn with his mouth. But every time he got close, the yarn moved away from him. The cat laughed silently as he watched the dog's futile attempts.\\n\\nThe dog soon realized that something was wrong. He looked behind him and saw that the yarn was attached to his tail. He tried to pull it off, but it was too tight. He felt angry and embarrassed. He wondered who did this to him. He sniffed the air and detected the cat's scent. He knew it was the cat who tricked him. He growled and ran towards the sofa where the cat was hiding.\\n\\nThe cat heard the dog's growl and saw him coming. He panicked and ran away from the sofa. He hoped to find a safe place to hide, but he was too late. The dog was faster and caught up with him. He grabbed the cat by the scruff of his neck and shook him hard. The cat yowled and scratched the dog's face. The dog barked and bit the cat's ear. They rolled on the floor, biting and clawing each other.\\n\\nThe noise they made woke up their owner, who was sleeping upstairs. She came down and saw them fighting. She was shocked and angry. She shouted at them to stop. She grabbed a broom and hit them lightly on their heads. They stopped fighting and looked at her with fear. She scolded them for being naughty and making a mess. She untied the yarn from the dog's tail and threw it away. She took them to the bathroom and cleaned their wounds. She told them to behave themselves and get along.\\n\\nThe cat and the dog felt ashamed of themselves. They realized that they had hurt each other and their owner. They apologized to each other and to their owner. They promised to be nicer to each other and share their things. They hugged each other and licked each other's faces.\\n\\nFrom that day on, they became friends. They played together, slept together, and ate together. They learned to respect each other's differences and appreciate each other's strengths. They were happy and content.\\n\\nThe end.\",\n",
       " 'output': '**Summary:**\\n\\n\"The Cat and the Dog\" is a story about a cat named Silvester and a dog who live together but don\\'t get along. Silvester, being clever and cunning, plays a prank on the dog by tying yarn to his tail. The dog, initially thinking it\\'s a toy, gets embarrassed and angry upon realizing the trick. A chase ensues, leading to a fight that awakens their owner. She stops the fight, scolds them, and tends to their wounds. Feeling ashamed, the cat and dog apologize to each other and their owner, promising to be nicer. From then on, they become friends, learning to respect and appreciate each other.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "# Import the string module\n",
    "import string\n",
    "\n",
    "# Define the function\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "  # Open the file in read mode\n",
    "  text = inputs[\"text\"]\n",
    "  # Create a table that maps punctuation characters to None\n",
    "  new_text = text.replace('cat', 'Silvester the Cat')\n",
    "  # Apply the table to the text and return the result\n",
    "  return {\"output_text\": new_text}\n",
    "\n",
    "with open(\"data/Cats&Dogs.txt\") as f: \n",
    "    cats_and_dogs= f.read() \n",
    "    \n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], \n",
    "    transform=rename_cat\n",
    ")\n",
    "template = \"\"\"Summarize this text: \n",
    "\n",
    "{output_text}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, \n",
    "llm_chain])\n",
    "\n",
    "sequential_chain.invoke(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Agents \n",
    "\n",
    "Agents are entities that drive decision-making within LLM-powered apps. \n",
    "Agent types: \n",
    "- Structured input ReAct. \n",
    "- OpenAI Functions. \n",
    "- Conversational \n",
    "- Self ask with search. \n",
    "- ReAct document store. \n",
    "- Plan-and-execute agents. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Conversational Applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a plain vanilla bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage, \n",
    "    HumanMessage, \n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.memory import ConversationBufferMemory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! Rome is a city rich in history, art, and culture. With only 2 days, you'll want to focus on the must-see attractions. Here's a suggested itinerary:\n",
      "\n",
      "### Day 1: Ancient Rome\n",
      "**Morning:**\n",
      "1. **Colosseum**: Start your day early to avoid crowds. The Colosseum is one of Rome's most iconic landmarks.\n",
      "2. **Roman Forum**: Just next to the Colosseum, the Roman Forum was the political and economic hub of the Roman Republic.\n",
      "\n",
      "**Afternoon:**\n",
      "3. **Palatine Hill**: Explore the ruins of ancient palaces and temples. It's a short walk from the Roman Forum.\n",
      "4. **Pantheon**: Head to the historic center to visit this ancient temple, now a church. It's one of the best-preserved ancient Roman buildings.\n",
      "\n",
      "**Evening:**\n",
      "5. **Piazza Navona**: Enjoy the beautiful fountains and architecture. It's a great place to relax and people-watch.\n",
      "6. **Dinner**: Try some authentic Italian cuisine in one of the nearby restaurants.\n",
      "\n",
      "### Day 2: Vatican City and Historic Sites\n",
      "**Morning:**\n",
      "1. **Vatican City**: Start early to visit St. Peter's Basilica and the Vatican Museums, including the Sistine Chapel.\n",
      "2. **St. Peter's Square**: Take some time to admire the architecture and the famous obelisk.\n",
      "\n",
      "**Afternoon:**\n",
      "3. **Castel Sant'Angelo**: Walk along the Tiber River to reach this historic castle, which offers great views of the city.\n",
      "4. **Trevi Fountain**: Make a wish and toss a coin into the famous fountain.\n",
      "\n",
      "**Evening:**\n",
      "5. **Spanish Steps**: Climb the steps and enjoy the view of the Piazza di Spagna.\n",
      "6. **Dinner**: End your day with a delicious Italian meal in one of the local trattorias.\n",
      "\n",
      "### Tips:\n",
      "- **Transportation**: Rome is very walkable, but you can also use public transportation like buses and the metro.\n",
      "- **Tickets**: Book your tickets for the Colosseum, Vatican Museums, and other popular attractions in advance to save time.\n",
      "- **Comfortable Shoes**: You'll be doing a lot of walking, so make sure to wear comfortable shoes.\n",
      "- **Hydration**: Carry a water bottle, especially during the summer months.\n",
      "\n",
      "Enjoy your trip to Rome!\n"
     ]
    }
   ],
   "source": [
    "chat = ChatMistralAI(\n",
    "    api_key=mistral_api_key,\n",
    "    model_name=\"mistral-large-latest\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that help the user to plan an optimized itinerary.\"),\n",
    "    HumanMessage(content=\"I'm going to Rome for 2 days, what can I visit?\")\n",
    "]\n",
    "\n",
    "output = chat(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! How are you today? I'm here and ready to chat about all sorts of things. Let's make this conversation interesting! How about I share a fun fact to start? Did you know that a day on Venus is longer than a year on Venus? It takes Venus about 243 Earth days to rotate once on its axis, but it only takes around 225 Earth days for Venus to orbit the Sun. Isn't that amazing? Now, it's your turn to share something or ask me a question.\n"
     ]
    }
   ],
   "source": [
    "# Adding memory \n",
    "memory = ConversationBufferMemory() \n",
    "conversation = ConversationChain(\n",
    "    llm=chat, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "print(conversation.run(\"Hi there!\").replace(\"\\\\n\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How are you today? I'm here and ready to chat about all sorts of things. Let's make this conversation interesting! How about I share a fun fact to start? Did you know that a day on Venus is longer than a year on Venus? It takes Venus about 243 Earth days to rotate once on its axis, but it only takes around 225 Earth days for Venus to orbit the Sun. Isn't that amazing? Now, it's your turn to share something or ask me a question.\n",
      "Human: What's the best place to live in Kenya?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great question! Determining the \"best\" place to live can depend on various factors such as personal preferences, lifestyle, and specific needs. However, I can tell you about some popular and highly-regarded places to live in Kenya.\n",
      "\n",
      "1. **Nairobi**: As the capital city, Nairobi offers a mix of modern amenities and cultural experiences. It has vibrant neighborhoods like Westlands, Karen, and Lavington, which are known for their upscale living, excellent schools, and numerous recreational facilities. However, traffic and pollution can be downsides.\n",
      "\n",
      "2. **Mombasa**: If you love coastal living, Mombasa is a fantastic choice. It offers beautiful beaches, a rich cultural heritage, and a more relaxed pace of life. Areas like Nyali and Diani are particularly popular for their scenic beauty and high-end properties.\n",
      "\n",
      "3. **Kisumu**: Located on the shores of Lake Victoria, Kisumu is known for its tranquil environment and stunning sunsets. It is a great place for those who enjoy outdoor activities and a slower pace of life compared to Nairobi.\n",
      "\n",
      "4. **Nakuru**: Often referred to as the cleanest town in East Africa, Nakuru is known for its picturesque landscapes and the nearby Lake Nakuru National Park, famous for its flamingos. It offers a balanced mix of urban amenities and natural beauty.\n",
      "\n",
      "5. **Naivasha**: This town is popular for its serene environment and proximity to Lake Naivasha. It is ideal for those who enjoy outdoor activities like hiking, boating, and birdwatching. The area is also known for its flower farms.\n",
      "\n",
      "Ultimately, the best place to live depends on what you prioritizewhether it's city life, natural beauty, or a mix of both. If you have specific preferences or needs, feel free to share, and I can give more tailored suggestions!\n"
     ]
    }
   ],
   "source": [
    "print(conversation.run(\"What's the best place to live in Kenya?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How are you today? I'm here and ready to chat about all sorts of things. Let's make this conversation interesting! How about I share a fun fact to start? Did you know that a day on Venus is longer than a year on Venus? It takes Venus about 243 Earth days to rotate once on its axis, but it only takes around 225 Earth days for Venus to orbit the Sun. Isn't that amazing? Now, it's your turn to share something or ask me a question.\n",
      "Human: What's the best place to live in Kenya?\n",
      "AI: That's a great question! Determining the \"best\" place to live can depend on various factors such as personal preferences, lifestyle, and specific needs. However, I can tell you about some popular and highly-regarded places to live in Kenya.\n",
      "\n",
      "1. **Nairobi**: As the capital city, Nairobi offers a mix of modern amenities and cultural experiences. It has vibrant neighborhoods like Westlands, Karen, and Lavington, which are known for their upscale living, excellent schools, and numerous recreational facilities. However, traffic and pollution can be downsides.\n",
      "\n",
      "2. **Mombasa**: If you love coastal living, Mombasa is a fantastic choice. It offers beautiful beaches, a rich cultural heritage, and a more relaxed pace of life. Areas like Nyali and Diani are particularly popular for their scenic beauty and high-end properties.\n",
      "\n",
      "3. **Kisumu**: Located on the shores of Lake Victoria, Kisumu is known for its tranquil environment and stunning sunsets. It is a great place for those who enjoy outdoor activities and a slower pace of life compared to Nairobi.\n",
      "\n",
      "4. **Nakuru**: Often referred to as the cleanest town in East Africa, Nakuru is known for its picturesque landscapes and the nearby Lake Nakuru National Park, famous for its flamingos. It offers a balanced mix of urban amenities and natural beauty.\n",
      "\n",
      "5. **Naivasha**: This town is popular for its serene environment and proximity to Lake Naivasha. It is ideal for those who enjoy outdoor activities like hiking, boating, and birdwatching. The area is also known for its flower farms.\n",
      "\n",
      "Ultimately, the best place to live depends on what you prioritizewhether it's city life, natural beauty, or a mix of both. If you have specific preferences or needs, feel free to share, and I can give more tailored suggestions!\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User:  \n",
      "AI System:  It seems like you might be considering or just curious about different places to live in Kenya. Do you have any particular interests or deal-breakers that could help narrow down the suggestions? For example, are you looking for a place with a vibrant nightlife, or perhaps a quiet town with good schools for children? Also, have you ever visited Kenya before? I'd love to hear about your experiences or expectations!\n",
      "\n",
      "If you're up for it, I can also share some unique facts or stories about each place. For instance, did you know that the Great Rift Valley, which runs through Nakuru and Naivasha, is believed to be the cradle of humanity? Many famous fossils, like the \"Turkana Boy,\" have been discovered in this region. It's pretty amazing to think that we can trace our origins back to Kenya!\n",
      "\n",
      "On the other hand, if you're leaning towards a more urban lifestyle, Nairobi has a lot to offer, including a thriving tech scene. It's often referred to as the \"Silicon Savannah\" because of its rapid growth in tech startups and innovations, like the mobile money transfer service M-Pesa.\n",
      "\n",
      "I'm here to provide as much information as you'd like, so just let me know how you'd like to proceed.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello! How are you today? I'm here and ready to chat about all sorts of things. Let's make this conversation interesting! How about I share a fun fact to start? Did you know that a day on Venus is longer than a year on Venus? It takes Venus about 243 Earth days to rotate once on its axis, but it only takes around 225 Earth days for Venus to orbit the Sun. Isn't that amazing? Now, it's your turn to share something or ask me a question.\n",
      "Human: What's the best place to live in Kenya?\n",
      "AI: That's a great question! Determining the \"best\" place to live can depend on various factors such as personal preferences, lifestyle, and specific needs. However, I can tell you about some popular and highly-regarded places to live in Kenya.\n",
      "\n",
      "1. **Nairobi**: As the capital city, Nairobi offers a mix of modern amenities and cultural experiences. It has vibrant neighborhoods like Westlands, Karen, and Lavington, which are known for their upscale living, excellent schools, and numerous recreational facilities. However, traffic and pollution can be downsides.\n",
      "\n",
      "2. **Mombasa**: If you love coastal living, Mombasa is a fantastic choice. It offers beautiful beaches, a rich cultural heritage, and a more relaxed pace of life. Areas like Nyali and Diani are particularly popular for their scenic beauty and high-end properties.\n",
      "\n",
      "3. **Kisumu**: Located on the shores of Lake Victoria, Kisumu is known for its tranquil environment and stunning sunsets. It is a great place for those who enjoy outdoor activities and a slower pace of life compared to Nairobi.\n",
      "\n",
      "4. **Nakuru**: Often referred to as the cleanest town in East Africa, Nakuru is known for its picturesque landscapes and the nearby Lake Nakuru National Park, famous for its flamingos. It offers a balanced mix of urban amenities and natural beauty.\n",
      "\n",
      "5. **Naivasha**: This town is popular for its serene environment and proximity to Lake Naivasha. It is ideal for those who enjoy outdoor activities like hiking, boating, and birdwatching. The area is also known for its flower farms.\n",
      "\n",
      "Ultimately, the best place to live depends on what you prioritizewhether it's city life, natural beauty, or a mix of both. If you have specific preferences or needs, feel free to share, and I can give more tailored suggestions!\n",
      "Human: \n",
      "AI: It seems like you might be considering or just curious about different places to live in Kenya. Do you have any particular interests or deal-breakers that could help narrow down the suggestions? For example, are you looking for a place with a vibrant nightlife, or perhaps a quiet town with good schools for children? Also, have you ever visited Kenya before? I'd love to hear about your experiences or expectations!\n",
      "\n",
      "If you're up for it, I can also share some unique facts or stories about each place. For instance, did you know that the Great Rift Valley, which runs through Nakuru and Naivasha, is believed to be the cradle of humanity? Many famous fossils, like the \"Turkana Boy,\" have been discovered in this region. It's pretty amazing to think that we can trace our origins back to Kenya!\n",
      "\n",
      "On the other hand, if you're leaning towards a more urban lifestyle, Nairobi has a lot to offer, including a thriving tech scene. It's often referred to as the \"Silicon Savannah\" because of its rapid growth in tech startups and innovations, like the mobile money transfer service M-Pesa.\n",
      "\n",
      "I'm here to provide as much information as you'd like, so just let me know how you'd like to proceed.\n",
      "Human: thank you\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User:  thank you\n",
      "AI System:  You're very welcome! I'm glad I could help. If you have any more questions about Kenya or any other topic, feel free to ask. I'm here to make our conversation enjoyable and informative.\n",
      "\n",
      "To keep our chat going, would you like to learn a bit of Swahili, one of the official languages of Kenya? For example, \"Jambo\" is a common greeting that means \"Hello.\" And \"Asante\" means \"Thank you.\" It's always fun to learn a few phrases from different languages!\n",
      "\n",
      "Or, if you prefer, we can switch gears entirely. How about a lighthearted topic? Have you watched any interesting movies or TV shows lately? I can provide recommendations if you're looking for something new to watch.\n",
      "\n",
      "The choice is yours! Let's make this conversation engaging and fun. What would you like to talk about or learn next?\n"
     ]
    }
   ],
   "source": [
    "# Interactive chat \n",
    "while True: \n",
    "    query = input(\"you: \")\n",
    "    if query == 'q': \n",
    "        break \n",
    "    output = conversation({'input': query})\n",
    "    print('User: ', query)\n",
    "    print(\"AI System: \", output['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "5 \n",
      "2.3 The need for Cloud Policy  \n",
      "The cloud computing landscape is robust and needs adequate policy and legal framework to ensure. \n",
      "The Kenya's existing policy and legal framework is inadequate and doesnt address all the issues \n",
      "and challenges related to cloud computing hence the need to come up with a comprehensive cloud \n",
      "policy.  While general ICT principles are outlined, there may be gaps in addressing the traditional \n",
      "on-premise challenges and opportunities associated with cloud computing. The establishment of a \n",
      "Cloud Policy in Kenya prese nts an opportunity to address gaps in the existing policy and legal \n",
      "framework related to cloud computing. By defining clear objectives, enhancing the legal \n",
      "framework, and promoting best practices, Kenya can position itself as a leader in cloud computing, \n",
      "driving innovation, efficiency, and competitiveness in the digital economy. \n",
      "The Kenya Cloud Policy builds upon the foundational principles and objectives outlined in \n",
      "National ICT Policy, which serves as the overarching framework guiding ICT development and \n",
      "governance in the country. This policy acknowledges the importance of lever aging cloud \n",
      "computing technologies to accelerate digital transformation, enhance service delivery, and drive \n",
      "economic growth, while ensuring alignment with existing legal frameworks and regulatory \n",
      "requirements. \n",
      "2.4 Current State \n",
      "Most organizations typically host their data and systems on -premise by managing their own IT\n",
      "\n",
      "MINISTRY OF INFORMATION, COMMUNICATIONS AND THE DIGITAL ECONOMY \n",
      "KENYA CLOUD POLICY \n",
      "2024\n",
      "\n",
      "19 \n",
      "4.2 Governance \n",
      "To ensure smooth implementation and achieve optimal results, a clearly defined governance \n",
      "structure is essential. Six primary roles have been identified to govern the implementation of the \n",
      "Kenya Cloud Policy. \n",
      "S/No Item Responsible Organ Roles \n",
      "1 Policy Body Ministry of Information, \n",
      "Communications and the \n",
      "Digital Economy. \n",
      " Defining the objectives and \n",
      "scope of the Kenya Cloud \n",
      "Policy. \n",
      " Setting the guidelines for the \n",
      "policy and publishing. \n",
      " Defining the roles and \n",
      "responsibilities of the \n",
      "different involved entities, in \n",
      "the context of the Cloud \n",
      "Policy. \n",
      " Updating and adjusting the \n",
      "Kenya Cloud Policy when \n",
      "required. \n",
      "2 Cloud Adoption \n",
      "Committee  \n",
      " \n",
      "Multi-agency committee to \n",
      "be constituted by the PS \n",
      "responsible for ICT \n",
      " Oversee cloud adoption \n",
      "across the different entities \n",
      "through pilots and supporting \n",
      "Entities during the migration \n",
      "process with technical and \n",
      "commercial expertise. \n",
      " Checking the cybersecurity, \n",
      "technical and commercial \n",
      "requirements.\n",
      "\n",
      "13 \n",
      "3 CHAPTER THREE: POLICY STATEMENT \n",
      "3.1 Objectives \n",
      "The Kenya Cloud Policy shall mandate all entities to prioritize cloud-based solutions when making \n",
      "ICT investments (procurement of hardware, software, renewal of existing software licenses, \n",
      "revamping existing ICT infrastructure including Data Centers). This prioritization aims to achieve \n",
      "the following key objectives: \n",
      "i. To accelerate adoption of green cloud computing technology  \n",
      "ii. To reduce Total Cost of Ownership of ICT infrastructure \n",
      "iii. To ensure robust Cybersecurity measures on data hosted on cloud. \n",
      "iv. To enable collaboration and interoperability among entities. \n",
      "v. To promote Data Residency and Sovereignty. \n",
      " \n",
      "3.2 Statements \n",
      "When making new IT investments, entities covered by this policy are required to consider the \n",
      "below stages: \n",
      " \n",
      "a) A New ICT investment includes procurement of new hardware and software, renewal of \n",
      "hardware and renewal of present software licenses. It is noteworthy that the entities falling \n",
      "under the scope of this policy must abide by the laws, regulations and controls relate d to \n",
      "data classification and other regulations regarding the location of hosting their data in any \n",
      "way.  \n",
      "b) If data is classified as top secret or secret, the government cloud service providers should \n",
      "be relied upon only if the technical and cybersecurity requirements are met. In the case that \n",
      "the government cloud service providers do not meet the technical and the cybersecurity\n",
      "Human: Give an overview of the kenya cloud policy.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided context, here's an overview of the Kenya Cloud Policy:\n",
      "\n",
      "**Purpose:**\n",
      "The Kenya Cloud Policy is designed to address gaps in the existing policy and legal framework related to cloud computing, aiming to drive innovation, efficiency, and competitiveness in the digital economy. It builds upon the principles and objectives outlined in the National ICT Policy.\n",
      "\n",
      "**Objectives:**\n",
      "1. To accelerate the adoption of green cloud computing technology.\n",
      "2. To reduce the Total Cost of Ownership of ICT infrastructure.\n",
      "3. To ensure robust cybersecurity measures on data hosted in the cloud.\n",
      "4. To enable collaboration and interoperability among entities.\n",
      "5. To promote Data Residency and Sovereignty.\n",
      "\n",
      "**Scope:**\n",
      "The policy mandates all entities to prioritize cloud-based solutions when making ICT investments, including procurement of hardware, software, renewal of existing software licenses, and revamping existing ICT infrastructure such as Data Centers.\n",
      "\n",
      "**Governance:**\n",
      "The policy defines a governance structure with primary roles, including:\n",
      "1. **Policy Body:** Led by the Ministry of Information, Communications and the Digital Economy, responsible for defining objectives, setting guidelines, and updating the policy.\n",
      "2. **Cloud Adoption Committee:** A multi-agency committee overseeing cloud adoption, supporting entities during migration, and checking cybersecurity, technical, and commercial requirements.\n",
      "\n",
      "**Compliance:**\n",
      "Entities covered by the policy must abide by laws, regulations, and controls related to data classification and hosting. For data classified as top secret or secret, government cloud service providers should be used only if they meet the technical and cybersecurity requirements.\n",
      "\n",
      "**Policy Statements:**\n",
      "The policy provides guidelines for entities to consider when making new IT investments, ensuring they comply with data classification and hosting regulations.\n"
     ]
    }
   ],
   "source": [
    "# Adding non-parametric knowledge \n",
    "from langchain_mistralai import MistralAIEmbeddings \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import ConversationalRetrievalChain \n",
    "from langchain.memory import ConversationBufferMemory \n",
    "\n",
    "mistral_embeddings = MistralAIEmbeddings(\n",
    "    api_key=mistral_api_key,\n",
    "    model=\"mistral-embed\"\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200 \n",
    ")\n",
    "raw_documents = PyPDFLoader(\"data/Kenya Cloud Policy.pdf\").load() \n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, mistral_embeddings)\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "llm = model \n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory, \n",
    "    verbose=True\n",
    ")\n",
    "print(qa_chain.run({'question': 'Give an overview of the kenya cloud policy.'}).replace(\"\\\\n\", '\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the bot agentic(from the langchain.agents.agent_toolkits). \n",
    "- create_retriever_tool: Creates a custom tool that acts as a retriever for an agent. \n",
    "- create_conversational_retrieval_agent: Initializes a conversational agent that is configured to work with retrievers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool, create_conversational_retrieval_agent\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    db.as_retriever(), \n",
    "    \"Kenya Cloud Policy\", \n",
    "    \"Search and return documents regarding the Kenya Cloud Policy.\"\n",
    ")\n",
    "tools = [tools]\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "agent_executor = create_conversational_retrieval_agent(llm, \n",
    "    tools, \n",
    "    memory_key='chat_history', \n",
    "    verbose=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
